Метод K-ближайшего соседа (k-nearest neighbors method, k-NN) – 
один из методов решения задачи классификации.

В основе k-NN лежит следующее правило: объект считается
принадлежащим тому классу, к которому относится большинство его
ближайших соседей. Под «соседями» здесь понимаются объекты, близкие к
исследуемому в том или ином смысле.

Предполагается, что объекты с близкими значениями одних признаков
будут близки и по другим признакам (т.е. относиться к одному и тому же
классу).

Согласно методу k-NN мы отнесём объект к тому классу, к которому
принадлежит большинство из k его ближайших соседей.

Если мы выберем значение k слишком малым, то есть опасность, что
единственным ближайшим объектом окажется «выброс», т.е. объект с
неправильно определённым классом, и он даст неверное решение. Казалось
бы, увеличивая значение параметра k, мы снижаем вероятность случайного
попадания на такие «выбросы» в качестве ближайших соседей исследуемого
объекта. Но здесь возникает другая опасность. Чтобы понять в чём она
заключается, рассмотрим случай, когда k равно общему числу объектов N.
Понятно, что тогда «победит» самый популярный (модальный) класс, и
расстояние до исследуемого объекта не будет играть вообще никакой роли.

Проблему выбора оптимального значения параметра k называют «bias-
variance tradeoff», т.е. «компромисс между «выбросами» и дисперсией». На

практике чаще всего полагают k = [√N].
